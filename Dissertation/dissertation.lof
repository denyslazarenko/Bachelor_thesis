\babel@toc {russian}{}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Classes, training set, and test set in text classification.\relax }}{7}{figure.caption.3}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Supervised learning work flow.\relax }}{8}{figure.caption.4}
\contentsline {figure}{\numberline {1.3}{\ignorespaces .\relax }}{10}{figure.caption.5}
\contentsline {figure}{\numberline {1.4}{\ignorespaces .\relax }}{10}{figure.caption.6}
\contentsline {figure}{\numberline {1.5}{\ignorespaces .\relax }}{11}{figure.caption.7}
\contentsline {figure}{\numberline {1.6}{\ignorespaces Confusion matrix\relax }}{12}{figure.caption.8}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Neural Network\relax }}{17}{figure.caption.10}
\contentsline {figure}{\numberline {2.2}{\ignorespaces The Skip-gram model architecture.\relax }}{20}{figure.caption.11}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Words representation\relax }}{22}{figure.caption.12}
\contentsline {figure}{\numberline {2.4}{\ignorespaces The CBOW model architecture.\relax }}{24}{figure.caption.13}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Convolution Neural Networks architecture for text classification\relax }}{25}{figure.caption.14}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Basic variables which are used in the convolution layer\relax }}{25}{figure.caption.15}
\contentsline {figure}{\numberline {2.7}{\ignorespaces ReLu activation function\relax }}{26}{figure.caption.16}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Max pulling layer\relax }}{27}{figure.caption.17}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Fully connected layer of CNN\relax }}{28}{figure.caption.18}
\contentsline {figure}{\numberline {2.10}{\ignorespaces Back propagation through max pulling layer\relax }}{29}{figure.caption.19}
\contentsline {figure}{\numberline {2.11}{\ignorespaces Back propagation through convolution layer\relax }}{30}{figure.caption.20}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Simplified event structure of data preprocessing\relax }}{37}{figure.caption.27}
\contentsline {figure}{\numberline {3.2}{\ignorespaces 1\relax }}{37}{figure.caption.29}
\contentsline {figure}{\numberline {3.3}{\ignorespaces 3\relax }}{38}{figure.caption.30}
\contentsline {figure}{\numberline {3.4}{\ignorespaces 4\relax }}{39}{figure.caption.31}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Architectures of Bi-LSTM models with 100 units \relax }}{40}{figure.caption.32}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Models train and validation categorical accuracy by epochs\relax }}{41}{figure.caption.33}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Models train and validation category crossentropy by epochs\relax }}{42}{figure.caption.34}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Models train and validation top k accuracy by epochs\relax }}{42}{figure.caption.35}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Models batch time by epochs\relax }}{43}{figure.caption.36}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Bi-LSTM 100 units. Histogram of output from forward recurrent layers (a); histogram of weights from backward recurrent layers (b)\relax }}{43}{figure.caption.37}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Bi-LSTM 100 units. Histogram of weights from first FFNN layer.\relax }}{44}{figure.caption.38}
\addvspace {10\p@ }
