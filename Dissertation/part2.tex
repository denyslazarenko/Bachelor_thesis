\chapter{Mathematical models and algorithms for text classification} \label{chapt2}

\section{Words representations} \label{sect2_1}


In supervised learning domain, to perform classification tasks, usually our goal is to find a parametrized model, best in its class: 

\begin{equation}
\label{eq:equation1}
A(X, \hat{w}): A(X, \hat{w}) \simeq f(X) \Leftrightarrow A(X, \hat{w}) = \operatorname*{arg\,min}_w \left\|A(X, w) - f(X)\right\|
\end{equation}

Where $X \in R^{ n\times m}$ - feature matrix ($n$ observations with $m$ features), $w \in R^{m}$ - vector of model parameters, $\hat{w}$ - "best" model parameters. However, as a candidate for X - all that we have is raw text input, algorithms can not use it as it is. In order to apply machine learning on textual data, firstly content should be transformed into specific numerical format, in another words it is necessary to form feature vectors. In Natural Language Processing automated feature extraction may be achieved in many ways.[тут вставить список литературы]

\subsection{Bag-of-Words Approach} \label{subsect2_1_1}

Bag-of-words - an unordered set of words, with their exact position ignored.\cite[p.641]{jurafsky}, 


\noindent In bag-of-words approach we work under the following assumptions:
\begin{itemize}
	\item The text can be analyzed without taking into account the word/token order.
	\item It is only necessary to know which words/tokens the text consists of and how many times.
\end{itemize}

Formally, there is a collection of texts $T_1, T_2, ... , T_n$. Unique tokens $w_1, w_2, ..., w_m$ are extracted to form a dictionary. Thus, each text $T_i$ is represented by feature vector $F_j = \{x_{ij},\ j \in [1,m]\}$, where $x_{ij}$ corresponds to number of occurrences of word $w_j$ in text $T_i$.

Example:
Our corpus represented by 2 texts:
["The sun is yellow", "The sky is blue"]

Our tokens are simple unigrams, therefore there are 6 unique words: {the, sun, is, yellow, sky, blue}. Then, given corpus is mapped to feature vectors:
$T_1=(1,1,1,1,0,0)$, $T_2=(1,0,1,0,1,1)$ 

\begin{table} [htbp]
	\centering
	\parbox{15cm}{\caption{Feature vector}\label{Ts0Sib_}}
	%  \begin{center}
	\begin{tabular}{| p{1cm} || p{1cm} | p{1cm} | p{1cm} | p{2cm} | p{1cm} | p{1cm}l |}
		\hline
		\hline
		Text & \centering the  & \centering sun  & \centering is  &\centering yellow &\centering sky  & \centering  blue & \\
		\hline
		$T_{1}$ &\centering  1   &\centering  1  &\centering  1   &\centering  1 &\centering  0 &\centering 0 &   \\
		$T_{2}$ &\centering  1   &\centering  0  &\centering  1   &\centering  0 &\centering  1 &\centering 1 &   \\
		\hline
		\hline
	\end{tabular}
	%  \end{center}
\end{table}

\noindent Benefits:
\begin{itemize}
	\item Despite its simplicity, demonstrate good results.
	\item Fast preprocessing.
	\item Built-in in many scientific/NLP libraries
\end{itemize}

\noindent Drawbacks:
\begin{itemize}
	\item Huge corpus usually leads to huge vocabulary size.
	\item Not memory-efficient: if we have corpus with 20 thousand texts then this textual corpus might spawn a dictionary with around 100 thousand elements. Thus, storing feature vectors as an array of type int32 would require 20000 x 100000 x 4 bytes $~$ 8GB in RAM.
	\item A bag of words is an orderless representation: throwing out spatial relationships between features leads to the fact that simplified model cannot let us to distinguish between sentences, built from the same words while having opposite meanings:"This paintings don't feel like ugly - buy them!" (positive) and "This paintings feel like ugly - don't buy them!" (negative) 
\end{itemize}

In order to capture dependencies between words \textbf{N-grams} technique can be used. N-gram is a sequence of $N$ basic tokens, which can be defined in different ways. 

\begin{enumerate}
	\item \textbf{Word n-grams - catches more semantics} :
	\begin{itemize}
		\item unigrams: "The sun is yellow." $\rightarrow$ ['The', 'sun', 'is' ...]
		\item bigrams: "The sun is yellow." $\rightarrow$
		['The sun', 'sun is' ...]
		\item 3-grams: "The sun is yellow." $\rightarrow$
		['The sun is ', 'sun is yellow']
	\end{itemize}
	\item \textbf{Character n-grams - helps to deal with misspelling}:
	\begin{itemize}
		\item 4-grams: "The sun is yellow." $\rightarrow$ ['The ', 'sun ', 'is y', 'ello' ...]
	\end{itemize}
	\item \textbf{Skip-n-gram - sequence of $N$ basic tokens, having distance of $\leq K$ tokens between them}  
	\begin{itemize}
		\item 1-skip-2-grams:: "The sun is yellow." $\rightarrow$ ['The is', 'sun yellow ']
	\end{itemize}
\end{enumerate}

In TF-IDF approach (term frequency - inverse document frequency), in addition to usual BoW-model, the following augmentation is made:

\subsection{TF-IDF  Approach} \label{subsect2_1_2}
Instead of just counting up the overlapping words, the algorithms applies a weight to each overlapping word. The TF weight measures how many times the word occurs in particular document while the IDF weight measures how many different documents a word occurs in and is thus a way of discounting function words. Since function words like the, of, etc., occur in many documents, their IDF is very low, while the IDF content words is high.\cite[p.647]{jurafsky} Formaly it can be defined: 



\begin{equation}
\label{eq:equation2_2}
\begin{cases} TF(w,T)=n_{Tw} \\ IDF(w, T)= log{\frac{N}{n_{w}}}\end{cases} \implies
TF\text{-}IDF(w, T) = n_{Tw}\ log{\frac{N}{n_{w}}} \ \ \ \ \forall w \in W
\end{equation}
\hfill \break 
where $T$ corresponds to current document (text), \hfill \break 
$w$ - selected word in document T, \hfill \break
$n_{Tw}$ - number of occurences of $w$ in text $T$, \hfill \break
$n_{w}$ - number of documents, containing word $w$, \hfill \break
$N$ - total number of documents in a corpus.

\begin{equation}
\label{eq:equation2_3}
\lim_{n_{w} \to N} {TF\text{-}IDF(w, T)} = 0
\end{equation}

\subsection{Embeddings} \label{subsect2_1_3}

Core idea:	A	word’s	meaning	is	given	by	the	words	that frequently	appear	close-by. 

тут вставить информацию про работі проведенные в данной области. 
https://arxiv.org/pdf/1301.3781.pdf
https://arxiv.org/pdf/1310.4546.pdf

1. \textbf{Skip-gram model}

\noindent To begin with key definitions of softmax~\ref{eq:equation2_4}  and sigmoid~\ref{eq:equation2_5} functions,

\begin{equation}
\label{eq:equation2_4}
\textrm{softmax}({\bf x})_{i} = \frac{e^{x_{i}}}{\sum_{j=1}{e^{x_{j}}}}
\end{equation}

\begin{equation}
\label{eq:equation2_5}
\textrm{softmax} = \sigma(z) = \frac{1}{1 + e^{-z}}.
\end{equation}

\noindent The gradient of sigmoid function is follows:

\begin{equation}
\label{eq:equation2_6}
\sigma^{\prime}(z) = \sigma(z)(1 - \sigma(z))
\end{equation}


\begin{figure}[ht] 
	\center
	\includegraphics [scale=0.5] {FCNN}
	\caption{Neural Network} 
	\label{img:FCNN}  
\end{figure}

\noindent where $x$ is one-hot input vector, $h$ - hidden layer, y is the one-hot label vector, and ŷ is the predicted probability vector for all classes.The neural network employs sigmoid activation function for the hidden layer, and softmax for the output layer and cross entropy cost ~\ref{eq:equation2_7} is used.

\begin{equation}
\label{eq:equation2_8}
\textrm{CE}({\bf y},{\bf\hat{y}}) = -\sum_{i}{y_{i}\log{\hat{y_{i}}}}
\end{equation}

Now, we will compute the gradient of cross entropy:

\begin{equation}
\label{eq:equation_CE_2}
\frac{\partial(\textrm{CE})}{\partial{\hat{y}_{i}}} = -\frac{y_{j}}{\hat{y}_{i}}
\end{equation}
That leads, 

\begin{equation}
\frac{\partial(\textrm{CE})}{\partial{\theta_{k}}} =  \frac{\partial(\textrm{CE})}{\partial{\hat{y}_{i}}}\frac{\partial{\hat{y}_{i}}}{\partial{\theta_{k}}} 
=-\frac{y_{j}}{\hat{y}_{i}}\frac{\partial{\hat{y}_{i}}}{\partial{\theta_{k}}}
\end{equation}

Calculating the partial derivative of $\hat{y_{i}}$ (where $i=k$)

\begin{equation}
\begin{multlined}
\label{eq:equation_CE_3}
\frac{\partial{\hat{y}_{i}}}{\partial{\theta_{i}}} =  \frac{\partial}{\partial{\theta_{i}}}\left( \frac{e^{\theta_{i}}}{\sum_{j=1}{e^{\theta_{j}}}}\right) =\\
= \frac{e^{\theta_{i}}}{\sum_{j=1}{e^{\theta_{j}}}} - \left(\frac{e^{\theta_{i}}}{\sum_{j=1}{e^{\theta_{j}}}}\right)^{2} = \\
= \hat{y}_{i}\cdot(1 - \hat{y}_{i})
\end{multlined}
\end{equation}

and (where $i\neq k$),

\begin{equation}
\label{eq:equation_CE_4}
\begin{multlined}
\frac{\partial{\hat{y}_{i}}}{\partial{\theta_{k}}} =  \frac{\partial}{\partial{\theta_{k}}}\left( \frac{e^{\theta_{i}}}{\sum_{j=1}{e^{\theta_{j}}}}\right) = \\
=-\left(\frac{e^{\theta_{i}}e^{\theta_{k}}}{\sum_{j=1}{e^{\theta_{j}}}}\right)
= - \hat{y}_{i}\hat{y}_{k}
\end{multlined}
\end{equation}

After combination of equations~\ref{eq:equation_CE_2},~\ref{eq:equation_CE_3},~\ref{eq:equation_CE_4}, 

\begin{equation}
\frac{\partial(\textrm{CE})}{\partial{\theta_{k}}} = \begin{cases}
-y_{j}(1 - \hat{y}_{k})&\text{ for }i=k \\
y_{j}\hat{y}_{k}&\text{ for }i\neq k
\end{cases}
\end{equation}

$y_{j}$ should be non-zero, $k=j$ and $y_{j}=1$, leads to,

\begin{equation}
\label{eq:equation_CE_5}
\frac{\partial(\textrm{CE})}{\partial{\theta_{j}}} = \begin{cases}
(\hat{y}_{j} - 1)&\text{ for }i=j \\
\hat{y}_{j}&\text{ for }i\neq j
\end{cases}
\end{equation}

Which is equivalent to,

\begin{equation}
\frac{\partial(\textrm{CE})}{\partial{\boldsymbol\theta}} = \bf{\hat{y}} - \bf{y}
\end{equation}

\noindent Forward propagation is as follows:
\begin{equation}
{\bf h} = \textrm{sigmoid}({\bf x\textrm{W}_{1} + b_{1}}) 
\end{equation}

\begin{equation}
\label{eq:equation2_7}
{\bf \hat{y}} = \textrm{softmax}({\bf h\textrm{W}_{2} + b_{2})}
\end{equation}

\noindent where $\bf{\textrm{W}}_i$ and $\bf{b}_{i}$ ($i\in\{1,2\}$) are
the weights and biases, respectively of the two layers.
\indent To optimize weights for each layer of neural network the back propagation algorithm is used. Therefore, it is necessary to calculate the gradients for each layer.
  
\noindent In order to simplify the notation used to solve the problem, define the following terms:
\begin{equation}
\label{eq:equation2_10}
\begin{multlined}
		 {\bf z}_{1}\equiv \quad{\bf x\textrm{W}_{1} + b_{1}} \\
		{\bf z}_{2}\equiv \quad{\bf h\textrm{W}_{2} + b_{2}}
\end{multlined}
\end{equation}

Starting with the results from ~\ref{eq:equation2_8}:

\begin{equation}
\frac{\partial{J}}{\partial{\bf z}_{2}} = \bf{\hat{y}} - \bf{y}
\end{equation}

and

\begin{equation}
\label{eq:equation2_11}
\frac{\partial{\bf z}_{2}}{\partial{{\bf h}}} = {\bf \textrm{W}^{\top}_{2}}
\end{equation}

Sigmoid ($\sigma$) derivative ~\ref{eq:equation2_6}:

\begin{equation}
\frac{\partial{{\bf h}}}{\partial{{\bf z}_{1}}}\equiv\sigma^{\prime}(z_{1})
\end{equation}

Combining these, and using $\cdot$ to denote element-wise product:

\begin{equation}
\frac{\partial{J}}{\partial{z_{i}}} = (\bf{\hat{y}} - \bf{y}){\bf \textrm{W}^{\top}_{2}}\cdot\sigma^{\prime}(z_{1})
\end{equation}

Finally, using the results from Equation~\ref{eq:equation2_11}:

\begin{equation}
\frac{\partial{J}}{\partial{{\bf W}^{(1)}}} = (\bf{\hat{y}} - \bf{y}){\bf \textrm{W}^{\top}_{2}}\cdot\sigma^{\prime}(z_{1})\cdot{\bf \textrm{X}^{\top}}
\end{equation}  

\begin{equation}
\frac{\partial{J}}{\partial{{\bf W}^{(2)}}} = (\bf{\hat{y}} - \bf{y}){\bf \textrm{h}^{\top}}
\end{equation}  

We have everything to update our weights: 


Now, turn definitely to skip-gram model:
\begin{figure}[ht] 
	\center
	\includegraphics [scale=0.5] {skip_gram_model}
	\caption{The Skip-gram model architecture. The training objective is to learn word vector representations
		that are good at predicting the nearby words.} 
	\label{img:skip_gram_model}  
\end{figure}



\noindent Assume you are given a predicted word vector ${\boldsymbol v}_{c}$ corresponding to the center word $c$ for
\texttt{skipgram}, and word prediction is made with the \texttt{softmax} function found in \texttt{word2vec} models
\begin{equation}
{\hat{\boldsymbol y}}_{\boldsymbol o} = p(~{\boldsymbol o} \mid {\boldsymbol c}~) = \frac{\exp{({\boldsymbol u}^{\top}_{o}{\boldsymbol v}_{c})}}{\sum^{\vert{W}\vert}_{j=1}\exp{({\boldsymbol u}^{\top}_{j}{\boldsymbol v}_{c})}}
\end{equation}
where $w$ denotes the $w$-th word and ${\boldsymbol u}_{w}$ ($w=1,...,\vert\textrm{W}\vert$)  are the `output' word vectors for all words in the vocabulary ($v^{\prime}_{w}$ in the lecture notes). Assume the cross entropy cost is applied to this prediction and word $o$ is the expected word (the $o$-th element of the one-hot label vector is one), derive the gradients with respect to ${\boldsymbol v}_{c}$.\\

\noindent \textbf{Hint:} It will be helpful to use notation from question 2. For instance, letting ${\hat{\boldsymbol y}}$ be the vector of the softmax
prediction for every word, ${\boldsymbol y}$ as the expected word vector, and the loss function
\begin{equation}
J_{\textrm{softmax-CE}}({\boldsymbol o}, {\boldsymbol v}_{c}, {\boldsymbol U}) = CE({\boldsymbol y},{\hat{\boldsymbol y}})
\end{equation}
where ${\boldsymbol U} = [ {\boldsymbol u}_{1}, {\boldsymbol u}_{2},...,{\boldsymbol u}_{\vert\textrm{W}\vert}]$ is the matrix of all the output vectors. \textit{Make sure you state the orientation of your vectors and matrices.}\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}
Applying cross-entropy cost to the above softmax probability defined above:
\begin{equation}
J =-\log{p} = - {\boldsymbol u}_{o}^{\top}{\boldsymbol v}_{c} + \log\sum^{\vert{V}\vert}_{j=1}\exp{({\boldsymbol u}_{j}^{\top}{\boldsymbol v}_{c})}
\end{equation}
Let $z_{j}={\boldsymbol u}_{j}^{\top}{\boldsymbol v}_{c}$, and $\delta^{i}_{j}$ be the indicator function (Kronecker delta), then
\begin{equation}
\frac{\partial J}{\partial{z_{k}}} = - \delta^{i}_{k} + \frac{\exp{({\boldsymbol u}_{i}^{\top}{\boldsymbol v}_{c})}}{\sum^{\vert{V}\vert}_{j=1}\exp{({\boldsymbol u}_{j}^{\top}{\boldsymbol v}_{c})}}
\end{equation}
Now, using the chain rule, we can calculate,
\begin{align}
\frac{\partial J}{\partial{{\boldsymbol v}_{c}}} =& \frac{\partial J}{\partial{{\boldsymbol z}}}\frac{\partial{{\boldsymbol z}}}{\partial{{\boldsymbol v}_{c}}} \\
=& \sum^{\vert{V}\vert}_{j=1}{\boldsymbol u}_{j}^{\top}\left(\frac{e^{z_{j}}}{\sum^{\vert{V}\vert}_{k=1}e^{z_{k}}} -  1\right) \\
=& \sum^{\vert{V}\vert}_{k=1}{\boldsymbol P}({\boldsymbol u}_{j} \vert {\boldsymbol v}_{c} ){\boldsymbol u}_{j} - {\boldsymbol u}_{j}
\end{align}


\noindent As in the previous problem, derive gradients for the `output' word vectors ${\boldsymbol u}_{w}$'s (including ${\boldsymbol u}_{o}$)\vspace{5mm}


\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}
This follows immediately from the chain rule:
\begin{align}
\frac{\partial J}{\partial{\boldsymbol u}_{j}} = &\frac{\partial J}{\partial{{\boldsymbol z}}}\frac{\partial{{\boldsymbol z}}}{\partial{\boldsymbol u}_{j}} \\
= &{\boldsymbol v}_{c}\left(\frac{\exp{({\boldsymbol u}^{\top}_{0}{\boldsymbol v}_{c})}}{\sum^{\vert{V}\vert}_{j=1}\exp{({\boldsymbol u}^{\top}_{j}{\boldsymbol v}_{c})}} - \delta^{0}_{j}\right)
\end{align}
